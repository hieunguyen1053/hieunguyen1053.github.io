{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17cd44a4-c4a1-45f3-8ae9-5623e2dd0db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d3af119-d19b-476c-a421-68f302daa1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tokenizer = Tokenizer.from_pretrained(\"Xenova/Llama-3.2-Tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4342eaa9-0473-4ebc-beb3-4e449bfe5efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_tokenizer = Tokenizer.from_pretrained(\"Xenova/gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c5620a8-dff0-4f21-976e-b943d7cd8e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Tokenizer.encode(self, sequence, pair=None, is_pretokenized=False, add_special_tokens=True)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_tokenizer.encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74a4540c-e046-48cc-a896-7a34e651049d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens ['###', ' **'] Num tokens: 2\n",
      "Tokens ['N', 'ội', ' dung', ' quy'] Num tokens: 4\n",
      "Tokens [' định', ' chính', ' của', ' Lu'] Num tokens: 4\n",
      "Tokens ['ật', ' Sở', ' hữu'] Num tokens: 3\n",
      "Tokens [' trí', ' tu', 'ệ', ' sửa'] Num tokens: 4\n",
      "Tokens [' đổi', ' ', '202', '5'] Num tokens: 4\n",
      "Tokens ['**\\n\\n', '-', ' **', 'Lu'] Num tokens: 4\n",
      "Tokens ['ật', ' hoá', ' vấn'] Num tokens: 3\n",
      "Tokens [' đề', ' trí', ' tu', 'ệ'] Num tokens: 4\n",
      "Tokens [' nhân', ' tạo', ' (', 'AI'] Num tokens: 4\n",
      "Tokens [')**', '  \\n', ' ', ' Lu'] Num tokens: 4\n",
      "Tokens ['ật', ' bổ', ' sung', ' khoản'] Num tokens: 4\n",
      "Tokens [' ', '5', ' vào', ' Điều'] Num tokens: 4\n",
      "Tokens [' ', '6', ',', ' giao'] Num tokens: 4\n",
      "Tokens [' cho', ' Chính', ' phủ', ' quy'] Num tokens: 4\n",
      "Tokens [' định', ' chi', ' tiết', ' việc'] Num tokens: 4\n",
      "Tokens [' phát', ' sinh', ' và', ' xác'] Num tokens: 4\n",
      "Tokens [' lập', ' quyền', ' sở', ' hữu'] Num tokens: 4\n",
      "Tokens [' trí', ' tu', 'ệ', ' đối'] Num tokens: 4\n",
      "Tokens [' với', ' các', ' đối', ' tượng'] Num tokens: 4\n",
      "Tokens [' được', ' tạo', ' ra', ' bằng'] Num tokens: 4\n",
      "Tokens [' hệ', ' thống', ' AI', '.'] Num tokens: 4\n",
      "Tokens [' Điều', ' này', ' cho', ' phép'] Num tokens: 4\n",
      "Tokens [' xác', ' lập', ' quyền', ' S'] Num tokens: 4\n",
      "Tokens ['HT', 'T', ' khi', ' AI'] Num tokens: 4\n",
      "Tokens [' tham', ' gia', ' tạo', ' ra'] Num tokens: 4\n",
      "Tokens [' sáng', ' tạo', ',', ' đồng'] Num tokens: 4\n",
      "Tokens [' thời', ' quy', ' định', ' việc'] Num tokens: 4\n",
      "Tokens [' sử', ' dụng', ' dữ', ' liệu'] Num tokens: 4\n",
      "Tokens [',', ' văn', ' bản', ' đã'] Num tokens: 4\n",
      "Tokens [' công', ' bố', ' cho', ' mục'] Num tokens: 4\n",
      "Tokens [' đích', ' nghiên', ' cứu'] Num tokens: 3\n",
      "Tokens [',', ' thử', ' nghiệm', ' và'] Num tokens: 4\n",
      "Tokens [' hu', 'ấn', ' luyện', ' AI'] Num tokens: 4\n",
      "Tokens [',', ' với', ' điều', ' kiện'] Num tokens: 4\n",
      "Tokens [' không', ' gây', ' ảnh', ' hưởng'] Num tokens: 4\n",
      "Tokens [' bất', ' hợp', ' lý', ' đến'] Num tokens: 4\n",
      "Tokens [' quyền', ' của', ' chủ', ' sở'] Num tokens: 4\n",
      "Tokens [' hữu', '.[', '1', ']['] Num tokens: 4\n",
      "Tokens ['2', ']\\n\\n', '-', ' **'] Num tokens: 4\n",
      "Tokens ['M', 'ở', ' rộng', ' và'] Num tokens: 4\n",
      "Tokens [' làm', ' rõ', ' giới', ' hạn'] Num tokens: 4\n",
      "Tokens [' quyền', ' vì', ' lợi', ' ích'] Num tokens: 4\n",
      "Tokens [' công', ' cộng', '**', '  \\n'] Num tokens: 4\n",
      "Tokens [' ', ' Điều', ' ', '7'] Num tokens: 4\n",
      "Tokens [' được', ' sửa', ' đổi', ','] Num tokens: 4\n",
      "Tokens [' nhấn', ' mạnh', ' việc'] Num tokens: 3\n",
      "Tokens [' khai', ' thác', ','] Num tokens: 3\n",
      "Tokens [' bảo', ' vệ', ' quyền', ' S'] Num tokens: 4\n",
      "Tokens ['HT', 'T', ' không', ' được'] Num tokens: 4\n"
     ]
    }
   ],
   "source": [
    "with open(\"response_chunk_0_50.txt\") as f:\n",
    "    for line in f:\n",
    "        data = line.replace(\"42\", \"\")\n",
    "        data = json.loads(data)\n",
    "        chunk = data[1]\n",
    "        result = llama_tokenizer.encode(chunk, add_special_tokens=False)\n",
    "        tokens = [llama_tokenizer.decode([id]) for id in result.ids]\n",
    "        print(\"Tokens\", tokens, \"Num tokens:\", len(result.ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d0efcb7-50f0-454d-ad9f-1f44cc331c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens ['###', ' **'] Num tokens: 2\n",
      "Tokens ['N', 'ội', ' dung', ' quy'] Num tokens: 4\n",
      "Tokens [' định', ' chính', ' của', ' Lu'] Num tokens: 4\n",
      "Tokens ['ật', ' S', 'ở', ' hữu'] Num tokens: 4\n",
      "Tokens [' trí', ' tu', 'ệ', ' sửa'] Num tokens: 4\n",
      "Tokens [' đổi', ' ', '202', '5'] Num tokens: 4\n",
      "Tokens ['**\\n\\n', '-', ' **', 'Lu'] Num tokens: 4\n",
      "Tokens ['ật', ' ho', 'á', ' vấn'] Num tokens: 4\n",
      "Tokens [' đề', ' trí', ' tu', 'ệ'] Num tokens: 4\n",
      "Tokens [' nhân', ' tạo', ' (', 'AI'] Num tokens: 4\n",
      "Tokens [')**', '  \\n', ' ', ' Lu'] Num tokens: 4\n",
      "Tokens ['ật', ' bổ', ' sung', ' khoản'] Num tokens: 4\n",
      "Tokens [' ', '5', ' vào', ' Điều'] Num tokens: 4\n",
      "Tokens [' ', '6', ',', ' giao'] Num tokens: 4\n",
      "Tokens [' cho', ' Chính', ' phủ', ' quy'] Num tokens: 4\n",
      "Tokens [' định', ' chi', ' tiết', ' việc'] Num tokens: 4\n",
      "Tokens [' phát', ' sinh', ' và', ' xác'] Num tokens: 4\n",
      "Tokens [' lập', ' quyền', ' sở', ' hữu'] Num tokens: 4\n",
      "Tokens [' trí', ' tu', 'ệ', ' đối'] Num tokens: 4\n",
      "Tokens [' với', ' các', ' đối', ' tượng'] Num tokens: 4\n",
      "Tokens [' được', ' tạo', ' ra', ' bằng'] Num tokens: 4\n",
      "Tokens [' hệ', ' thống', ' AI', '.'] Num tokens: 4\n",
      "Tokens [' Điều', ' này', ' cho', ' phép'] Num tokens: 4\n",
      "Tokens [' xác', ' lập', ' quyền', ' S'] Num tokens: 4\n",
      "Tokens ['HT', 'T', ' khi', ' AI'] Num tokens: 4\n",
      "Tokens [' tham', ' gia', ' tạo', ' ra'] Num tokens: 4\n",
      "Tokens [' sáng', ' tạo', ',', ' đồng'] Num tokens: 4\n",
      "Tokens [' thời', ' quy', ' định', ' việc'] Num tokens: 4\n",
      "Tokens [' sử', ' dụng', ' dữ', ' liệu'] Num tokens: 4\n",
      "Tokens [',', ' văn', ' bản', ' đã'] Num tokens: 4\n",
      "Tokens [' công', ' bố', ' cho', ' mục'] Num tokens: 4\n",
      "Tokens [' đ', 'ích', ' nghiên', ' cứu'] Num tokens: 4\n",
      "Tokens [',', ' thử', ' nghiệm', ' và'] Num tokens: 4\n",
      "Tokens [' hu', 'ấn', ' luyện', ' AI'] Num tokens: 4\n",
      "Tokens [',', ' với', ' điều', ' kiện'] Num tokens: 4\n",
      "Tokens [' không', ' gây', ' ảnh', ' hưởng'] Num tokens: 4\n",
      "Tokens [' bất', ' hợp', ' lý', ' đến'] Num tokens: 4\n",
      "Tokens [' quyền', ' của', ' chủ', ' sở'] Num tokens: 4\n",
      "Tokens [' hữu', '.[', '1', ']['] Num tokens: 4\n",
      "Tokens ['2', ']\\n\\n', '-', ' **'] Num tokens: 4\n",
      "Tokens ['M', 'ở', ' rộng', ' và'] Num tokens: 4\n",
      "Tokens [' làm', ' rõ', ' giới', ' hạn'] Num tokens: 4\n",
      "Tokens [' quyền', ' vì', ' lợi', ' ích'] Num tokens: 4\n",
      "Tokens [' công', ' cộng', '**', '  \\n'] Num tokens: 4\n",
      "Tokens [' ', ' Điều', ' ', '7'] Num tokens: 4\n",
      "Tokens [' được', ' sửa', ' đổi', ','] Num tokens: 4\n",
      "Tokens [' nh', 'ấn', ' mạnh', ' việc'] Num tokens: 4\n",
      "Tokens [' khai', ' th', 'ác', ','] Num tokens: 4\n",
      "Tokens [' bảo', ' vệ', ' quyền', ' S'] Num tokens: 4\n",
      "Tokens ['HT', 'T', ' không', ' được'] Num tokens: 4\n"
     ]
    }
   ],
   "source": [
    "with open(\"response_chunk_0_50.txt\") as f:\n",
    "    for line in f:\n",
    "        data = line.replace(\"42\", \"\")\n",
    "        data = json.loads(data)\n",
    "        chunk = data[1]\n",
    "        result = gpt_tokenizer.encode(chunk, add_special_tokens=False)\n",
    "        tokens = [gpt_tokenizer.decode([id]) for id in result.ids]\n",
    "        print(\"Tokens\", tokens, \"Num tokens:\", len(result.ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2847184-6a32-4a96-b73c-f5f03e3cc48e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
